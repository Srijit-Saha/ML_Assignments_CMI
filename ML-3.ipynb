{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIVoVuh3zeKL",
        "outputId": "8c67f72d-5389-49e3-beaa-944b5dde2994"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=73f223b04712717fdb77b789114467739530640628948ee2b479b14e197dffbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.60.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uMZm6aPG8wwR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models, layers, losses, metrics, callbacks, datasets\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from joblib import dump, load\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "seed=42\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing data:\n",
        "Here we are importing the data.The data is normalized.We also check the shape of the train data and test data.Lastly we flatten the test and the training data."
      ],
      "metadata": {
        "id": "hR5cdxG-4qBe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE77aRS09LNA",
        "outputId": "09580170-7379-447e-b014-a1991b36c571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "X-train shape: (60000, 28, 28)\n",
            "y-train shape: (60000,)\n",
            "X-test shape: (10000, 28, 28)\n",
            "y-test shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "(X_train,y_train),(X_test,y_test)=datasets.fashion_mnist.load_data()\n",
        "class_names=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt',\n",
        "            'Sneaker','Bag','Ankle Boot']\n",
        "# Normalizing\n",
        "X_train, X_test= X_train/255.0, X_test/255.0\n",
        "\n",
        "# Checking shape of the data\n",
        "print(f\"X-train shape: {X_train.shape}\")\n",
        "print(f\"y-train shape: {y_train.shape}\")\n",
        "print(f\"X-test shape: {X_test.shape}\")\n",
        "print(f\"y-test shape: {y_test.shape}\")\n",
        "\n",
        "# Flattening data for Logistic Regression\n",
        "X_train_flatten= X_train.reshape(X_train.shape[0],-1)\n",
        "y_train_flatten= y_train.reshape(y_train.shape[0],-1)\n",
        "X_test_flatten= X_test.reshape(X_test.shape[0],-1)\n",
        "y_test_flatten= y_test.reshape(y_test.shape[0],-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we check on an item."
      ],
      "metadata": {
        "id": "0554Qgxd5GOE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "smuNHcV39w3p",
        "outputId": "3205a6df-6bad-4c9d-b326-9d0c1615936f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is a  Shirt\n"
          ]
        }
      ],
      "source": [
        "index=3000\n",
        "plt.imshow(X_train[index],cmap='binary')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print('It is a ',class_names[y_train[index]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here first we train a logistic regression\n",
        "model on a sample of  labeled instances from the  dataset such that the sample size is like 10,30,..90.We observe that the score of the fit is very low for all of the cases.We try to increase them by using clustering.We cluster the training set into  clusters of size k.This increases the accuracy."
      ],
      "metadata": {
        "id": "H1BQbpmu5V-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from memory_profiler import profile\n",
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "id": "pEaXtrkXAt3C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "%memit\n",
        "t1 = time.perf_counter()\n",
        "k_cluster = [i for i in range(10,110,20)]\n",
        "diff=[]#vector containing the increment in accuracy after doing clustering\n",
        "for k in k_cluster:\n",
        "  #logistic regression on sample of size k\n",
        "  log_reg_n = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", random_state=seed)\n",
        "  log_reg_n.fit(X_train_flatten[:k], y_train_flatten.ravel()[:k])\n",
        "  #k means clustering on the data\n",
        "  kmeans1 = KMeans(init='k-means++',n_clusters=k, random_state=seed)\n",
        "  X_items_dist = kmeans1.fit_transform(X_train_flatten)\n",
        "  representative_items_idx = np.argmin(X_items_dist, axis=0)\n",
        "  X_representative_items = X_train_flatten[representative_items_idx]\n",
        "  y_representative_items = np.squeeze(y_train_flatten[representative_items_idx]).astype('int32')\n",
        "  log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", random_state=seed)\n",
        "  log_reg.fit(X_representative_items, y_representative_items)\n",
        "  t1 = log_reg_n.score(X_test_flatten, y_test_flatten.ravel())\n",
        "  t2 = log_reg.score(X_test_flatten,y_test_flatten)\n",
        "\n",
        "  diff.append((t2-t1))\n",
        "  print(\"k:\",k)\n",
        "  print(\" accuracy of logistic regression \",t1)\n",
        "  print(\" accuracy of logistic regression after clustering\",t2)\n",
        "tw = time.perf_counter() - t1\n",
        "print(tw, \"seconds\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmX78J97cSHX",
        "outputId": "e322dc98-b44a-4347-bb60-1baad609233d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 1057.15 MiB, increment: 0.05 MiB\n",
            "k: 10\n",
            " accuracy of logistic regression  0.3736\n",
            " accuracy of logistic regression after clustering 0.483\n",
            "k: 30\n",
            " accuracy of logistic regression  0.5716\n",
            " accuracy of logistic regression after clustering 0.635\n",
            "k: 50\n",
            " accuracy of logistic regression  0.6618\n",
            " accuracy of logistic regression after clustering 0.6631\n",
            "k: 70\n",
            " accuracy of logistic regression  0.691\n",
            " accuracy of logistic regression after clustering 0.659\n",
            "k: 90\n",
            " accuracy of logistic regression  0.6825\n",
            " accuracy of logistic regression after clustering 0.6928\n",
            "995.238330875 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff.index(max(diff))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5GPToyvcva5",
        "outputId": "5664203d-b898-4fda-a42e-9abb499bba2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we observe that the increment is maximum when k=10.So we further try  propagate the labels to all the other instances in the same cluster.We then try to increase the accuracy even by propagating the labels to the 25% of the instances that are closest to the centroids."
      ],
      "metadata": {
        "id": "8OFMEzgg8lEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=10\n",
        "%memit\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "y_train_propagated = np.empty(len(X_train_flatten), dtype=np.int32)\n",
        "for i in range(k):\n",
        "    y_train_propagated[kmeans1.labels_==i] = y_representative_items[i]\n",
        "log_reg4 = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=1000, random_state=seed)\n",
        "log_reg4.fit(X_train_flatten, y_train_propagated)\n",
        "\n",
        "percentile_closest = 25\n",
        "\n",
        "X_cluster_dist = X_items_dist[np.arange(len(X_train_flatten)), kmeans1.labels_]\n",
        "for i in range(k):\n",
        "    in_cluster = (kmeans1.labels_ == i)\n",
        "    cluster_dist = X_cluster_dist[in_cluster]\n",
        "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
        "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
        "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
        "\n",
        "partially_propagated = (X_cluster_dist != -1)\n",
        "X_train_partially_propagated = X_train_flatten[partially_propagated]\n",
        "y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
        "\n",
        "log_reg5 = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=1000, random_state=seed)\n",
        "log_reg5.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
        "  \n",
        "  \n",
        "\n",
        "print(\"k:\",k)\n",
        "print(\" accuracy of logistic regression after label propagation\",log_reg4.score(X_test_flatten,y_test_flatten))\n",
        "print(\" accuracy of logistic regression after 25% closest instances are labelled \",log_reg5.score(X_test_flatten,y_test_flatten))\n",
        "tw = time.perf_counter() - t1\n",
        "print(tw, \"seconds\")\n",
        "from memory_profiler import profile\n",
        "%load_ext memory_profiler\n",
        "%memit dav = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=1000, random_state=seed).fit(X_train_partially_propagated, y_train_partially_propagated)\n"
      ],
      "metadata": {
        "id": "vDejIfXjkE3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2249daf4-e9a2-4a95-c748-aa1d61ece303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 1845.43 MiB, increment: -0.01 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(y_train_partially_propagated == y_train[partially_propagated])"
      ],
      "metadata": {
        "id": "vYfdchtOBinp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We try to now experiment with k=100"
      ],
      "metadata": {
        "id": "83bvTroR8buQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=100\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "%%memit\n",
        "log_reg_n = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", random_state=seed)\n",
        "log_reg_n.fit(X_train_flatten[:k], y_train_flatten.ravel()[:k])\n",
        "\n",
        "kmeans1 = KMeans(init='k-means++',n_clusters=k, random_state=seed)\n",
        "X_items_dist = kmeans1.fit_transform(X_train_flatten)\n",
        "representative_items_idx = np.argmin(X_items_dist, axis=0)\n",
        "X_representative_items = X_train_flatten[representative_items_idx]\n",
        "y_representative_items = np.squeeze(y_train_flatten[representative_items_idx]).astype('int32')\n",
        "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", random_state=seed)\n",
        "log_reg.fit(X_representative_items, y_representative_items)\n",
        "  \n",
        "  \n",
        "y_train_propagated = np.empty(len(X_train_flatten), dtype=np.int32)\n",
        "for i in range(k):\n",
        "    y_train_propagated[kmeans1.labels_==i] = y_representative_items[i]\n",
        "log_reg4 = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=1000, random_state=seed)\n",
        "log_reg4.fit(X_train_flatten, y_train_propagated)\n",
        "\n",
        "percentile_closest = 25\n",
        "\n",
        "X_cluster_dist = X_items_dist[np.arange(len(X_train_flatten)), kmeans1.labels_]\n",
        "for i in range(k):\n",
        "    in_cluster = (kmeans1.labels_ == i)\n",
        "    cluster_dist = X_cluster_dist[in_cluster]\n",
        "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
        "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
        "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
        "\n",
        "partially_propagated = (X_cluster_dist != -1)\n",
        "X_train_partially_propagated = X_train_flatten[partially_propagated]\n",
        "y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
        "\n",
        "log_reg5 = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=1000, random_state=seed)\n",
        "log_reg5.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
        "  \n",
        "  \n",
        "\n",
        "print(\"k:\",k)\n",
        "print(\" accuracy of logistic regression \",log_reg_n.score(X_test_flatten, y_test_flatten.ravel()))\n",
        "print(\" accuracy of logistic regression after clustering\",log_reg.score(X_test_flatten,y_test_flatten))\n",
        "print(\" accuracy of logistic regression after label propagation\",log_reg4.score(X_test_flatten,y_test_flatten))\n",
        "print(\" accuracy of logistic regression after 25% closest instances are labelled \",log_reg5.score(X_test_flatten,y_test_flatten))\n",
        "\n",
        "tw = time.perf_counter() - t1\n",
        "print(tw, \"seconds\")\n"
      ],
      "metadata": {
        "id": "oYOe0MpS8aoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(y_train_partially_propagated == y_train[partially_propagated])"
      ],
      "metadata": {
        "id": "nGEf9N4cBrMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%memit dav = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=1000, random_state=seed).fit(X_train_partially_propagated, y_train_partially_propagated)\n"
      ],
      "metadata": {
        "id": "QyhgKhyI_ZUs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of assignment3_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}